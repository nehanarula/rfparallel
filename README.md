# Parallel Random Forest: Design, Implementation, and Evaluation

## ğŸ“Œ Overview

This project implements a **Random Forest classifier from scratch** with multiple **parallelization strategies** and evaluates their performance in terms of **runtime speedup** and **classification quality**.

The goal is to study **how different parallelization approaches behave in practice**, especially under:
- Large datasets
- Class imbalance
- Realistic preprocessing overheads

The implementation avoids using `sklearn`â€™s built-in `RandomForestClassifier` for training and instead builds trees manually using `DecisionTreeClassifier`.

---

## ğŸš€ Parallelization Strategies Implemented

1. **Sequential**
2. **Tree Parallelism**
3. **Data Parallelism**
4. **Hybrid Parallelism**

Each approach builds the **same number of trees**, enabling fair comparison.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ group_63_parallel_random_forest.py
â”œâ”€â”€ rf_training.log
â”œâ”€â”€ outputs/
    â”œâ”€â”€ confusion_matrix.png
    â”œâ”€â”€ rf_performance_comparison.png
â”œâ”€â”€ README.md
```

---

## ğŸ“Š Evaluation Metrics

- Accuracy
- Precision
- Recall
- F1-score
- Macro and weighted averages

Support values are shown **only for individual classes**.  
Accuracy is reported separately.

---

## ğŸ“ Logging

- Uses Python `logging`
- Single shared log file
- Timestamped entries

---

## â–¶ï¸ How to Run

```bash
python3 group_63_parallel_random_forest.py
```

Dependencies:
- numpy
- pandas
- scikit-learn

